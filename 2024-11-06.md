# relazione utile
Se il problema di PL ammette soluzione ottima allora esiste almeno una SBA che ha i coefficienti di costo ridotto maggiori o uguali a zero.
# fase 1 metodo simplesso
Per certificare che l'insieme ammissibile e' vuoto e quindi deve creare una SBA da cui partire, la fase 1 introduce delle variabili artificiali e una definizione di problema ausiliario.

Partendo da un problema normale in forma standard
$$
\begin{align}
\text{min } c^T x \\
Ax=b \\
x\geq 0
\end{align}
$$
posso dire senza perdere di generalità che 
$$
b \geq 0
$$
perché se non lo fosse, posso cambiarne il segno dell'equazione.
Otteniamo adesso il problema ausiliario, ossia il problema in PL in (n+m) variabili:
$$
\begin{align}
\text{min } \alpha_{1}+\alpha_{2}+\dots+\alpha_{m} \\
\alpha_{11}x_{1}+\dots+a_{1n}x_{n}+\alpha_{1}=b_{1} \\
\alpha_{21}x_{1}+\dots+a_{2n}x_{n}+\alpha_{2}=b_{2} \\
\vdots \\
\alpha_{m1}x_{1}+\dots+a_{mn}x_{m}+\alpha_{m}=b_{m} \\ \\


x\geq 0 ; \alpha \geq 0
\end{align}
$$

Scrivendolo tramite matrici
$$
\begin{align}
\text{min } 1^T\alpha \\
Ax+I\alpha=b \\
x\geq 0, \alpha \geq 0
\end{align}
$$
- Ha insieme ammissibile vuoto? Vediamo se troviamo una soluzione ammissibile.
No, basta prendere:
$$
\begin{pmatrix}
x \\
\alpha
\end{pmatrix}\implies \begin{pmatrix}
0 \\
b
\end{pmatrix}
$$

che verifica l'equazione $Ax+I\alpha=b$.
- E' illimitato inferiormente?
No, non può essere illimitato inferiormente essendo che $\alpha \geq 0$ e la funzione obiettivo e' $1^T\alpha$.

Essendo non verificate queste due condizioni, per il [[Teorema fondamentale della PL riferito alle SBA]] $\implies$ il problema ammette soluzione ottima.

Indichiamo quindi con $z^*$ il valore della funzione obiettivo del problema ausiliario all'ottimo.
Quindi se come soluzione ottima del problema prendiamo:
$$
\begin{pmatrix}
x^* \\
\alpha^*
\end{pmatrix}\implies z^*=1^T\alpha^* \geq 0.
$$
## Proposizione (necessaria e sufficiente)

> [!Tip] Proposizione
> L'insieme ammissibile del problema originario e' non vuoto se e solo se $z^*=0$

- Ricordiamo
	- per dimostrare che la condizione e' ==necessaria==, abbiamo come ipotesi il fatto che l'insieme risulta non vuoto, la tesi e' che quindi anche $z^*$ deve essere 0 se vale l'ipotesi.
	- per dimostrare che la condizione e' ==sufficiente==, dimostriamo che se vale $z^*$ allora sicuramente l'insieme e' ammissibile risulta vuoto. 

### dimostrazione condizione necessaria
Hyp: insieme ammissibile del problema originario non vuoto.
Questo implica che
$$
\begin{align}
\exists \bar{x}:A\bar{x}=b \\
\bar{x}\geq 0
\end{align}
$$
allora prendo come punto ammissibile per il problema ausiliario
$$
\begin{pmatrix}
\bar{x} \\
0
\end{pmatrix}\implies A\bar{x}+I\cdot 0 =b
$$
dove la funzione obiettivo del problema ausiliario rispetto questo punto, risulterà
$$
\bar{z}=0
$$
per definizione allora $\bar{z} = z^*$ essendo 0.
### dimostrazione condizione sufficiente
Hyp: $z^*=0$.

$$
\begin{pmatrix}
x^* \\
\alpha^*
\end{pmatrix} \text{ soluzione ottima problema ausiliario}
$$
Quindi
$$
z^*=0\implies\alpha^*=0\implies Ax^*+I\cdot 0=b
$$
Cosi abbiamo ottenuto
$$
Ax^*=b \implies x^* \text{ punto ammissibile problema originario}
$$

### nella pratica
Dato 
$$
\begin{align}
\text{min } 1^T\alpha \\
Ax+I\alpha=b \\
x\geq 0, \alpha \geq 0
\end{align}
$$
Possiamo far partire la fase 2?
$$
\tilde{A}=\begin{pmatrix}
A & I
\end{pmatrix}
$$
Notiamo che abbiamo $I$ che possiamo prendere per ottenere una matrice di base partendo da questo problema, sempre.
$$
\begin{pmatrix}
x \\
\alpha
\end{pmatrix}\implies
\begin{pmatrix}
0 \\
b
\end{pmatrix} \text{ e' una SBA del problema ausiliario}
$$
Questo implica che posso far partire la fase 2 del metodo del simplesso applicata al problema ausiliario in quanto mi viene "regalata" sempre una SBA associata al problema ausiliario da cui posso far partire la seconda fase.

Quindi adesso sfruttando la proposizione precedente possiamo dire:
- Se $z^* >0 \implies$ problema originario ha insieme ammissibile vuoto
- Se $z^* = 0 \implies \alpha^* = 0$
	- Se tutte le variabili $\alpha$ sono uscite dalla base $\implies$ e' disponibile una SBA del problema originario $x^* = \begin{pmatrix}Ab^{-1}b\\ 0\end{pmatrix}$
	- Se qualche variabile $\alpha_{i}^*$ e' rimasta in base $\implies$ SBA degenere del problema ausiliario
		- Svogliamo uno scambio degenere in cui proviamo a far entrare in base variabili originarie al posto di variabili ausiliarie. Di fatto stiamo rappresentando sempre lo stesso punto ma stiamo cercando di ottenere una soluzione di base non degenere. Certe volte questo e' impossibile.
		- Se lo scambio degenere e' impossibile, la riga corrispondente alla variabile artificiale rimasta in base e' combinazione lineare delle altre e qiundi puo' essere eliminata.
# Teoria della dualità
Si divide in 
- caso della programmazione lineare (noi studieremo questo)
- caso della programmazione non lineare
## caso programmazione lineare
Per ogni problema primale, si può costruire un problema duale, che sarà un problema di minimizzazione (se il primale è di massimizzazione) oppure di massimizzazione (se il primale è di minimizzazione). 

### Concetto di Rilassamento di un Problema

Quando parliamo di **rilassamento di un problema** in ottimizzazione, ci riferiamo a una tecnica per semplificare un problema complesso, rendendolo più facile da risolvere. 

#### Definizione Formale

Dato un problema di ottimizzazione (I) nella forma:
$$
\min f(x) \quad \text{soggetto a} \quad x \in A
$$
definiamo un secondo problema (II) nella forma:
$$
\min g(x) \quad \text{soggetto a} \quad x \in B
$$
e diciamo che (II) è un **rilassamento di (I)** se:

1. **Insieme di fattibilità esteso**: $A \subseteq B$, cioè l’insieme di vincoli di (II) include o "rilassa" quelli di (I). Questo significa che ogni soluzione ammissibile di (I) è anche ammissibile per (II), ma non necessariamente viceversa.
   
2. **Funzione obiettivo rilassata**: Per ogni $x \in A$, si ha $g(x) \leq f(x)$. Questo implica che la funzione obiettivo del rilassamento (II) sottostima o uguaglia quella del problema originale per tutte le soluzioni ammissibili del problema originario.

In pratica, risolvere il rilassamento significa ottenere una stima del valore ottimale di $f(x)$, e questa soluzione può fornire una "frontiera" o limite inferiore (nel caso di minimizzazione) che ci dice quanto in basso potremmo spingere la funzione obiettivo di (I).

Un esempio comune è il **rilassamento continuo**: se il problema (I) è un problema di ottimizzazione su variabili intere, possiamo creare un rilassamento continuo permettendo alle variabili di assumere valori reali (cioè, togliamo i vincoli di integrità).

### Rilassamento Lagrangiano

Il **rilassamento lagrangiano** è una tecnica di rilassamento che utilizza i moltiplicatori di Lagrange per "rilassare" alcuni dei vincoli del problema. In questo caso, certi vincoli difficili vengono incorporati direttamente nella funzione obiettivo, associando a ciascuno di essi un moltiplicatore lagrangiano. Questo trasforma il problema originale in uno più facile da risolvere, pur mantenendo informazioni preziose sui vincoli rilassati.

#### Costruzione del Rilassamento Lagrangiano

Supponiamo di avere un problema (I) nella forma:
$$
\min f(x) \quad \text{soggetto a} \quad h_i(x) \leq 0, \, \forall i=1,\dots,m, \quad x \in X
$$
dove $h_i(x) \leq 0$ rappresenta un insieme di vincoli difficili da gestire.

Nel rilassamento lagrangiano, questi vincoli vengono incorporati nella funzione obiettivo con i **moltiplicatori lagrangiani** $ \lambda_i \geq 0 $ per ogni vincolo $h_i(x)$. La funzione obiettivo del rilassamento lagrangiano diventa quindi:
$$
L(x, \lambda) = f(x) + \sum_{i=1}^m \lambda_i h_i(x)
$$
Il problema rilassato diventa:
$$
\min_{x \in X} L(x, \lambda)
$$
dove $\lambda$ è un vettore di moltiplicatori che viene scelto per ottenere il miglior limite possibile per il problema originale.

#### Interpretazione e Utilizzo

1. **Limite Inferiore**: Per ogni scelta dei moltiplicatori $\lambda \geq 0$, la soluzione del rilassamento lagrangiano fornisce un limite inferiore al problema originale (nel caso di minimizzazione). Variando $\lambda$, possiamo migliorare questo limite e avvicinarci al valore ottimale di $f(x)$.

2. **Ottimizzazione Duale**: Si può pensare a risolvere il problema lagrangiano come trovare il valore migliore dei moltiplicatori $\lambda$ per ottenere il limite inferiore più alto possibile (massimizzare il limite inferiore), ottenendo così il **problema duale**.

In sintesi, il rilassamento lagrangiano consente di semplificare un problema mantenendo informazioni utili sui vincoli rilassati tramite i moltiplicatori di Lagrange, e aiuta a ottenere buone approssimazioni al problema originale.


### Rilassamento Lagrangiano per problema di PL in forma standard
Dato un problema di PL in forma standard, facendo riferimento al rilassamento lagrangiano:
$$
\begin{align}
h(x) &  \to Ax=b \\
s  & \to x\geq 0
\end{align}
$$
Fissato $\lambda \in \mathbb{R}^m$
$$
L(\lambda)=\text{min }c^T x-\lambda (Ax-b)=\text{min }\lambda^Tb+c^Tx-\lambda^TAx=\lambda^Tb+min(c^T-\lambda^TA)x
$$
Quindi il duale:
$$
\text{max }L(\lambda)
$$
$1^o$ caso: 
$$
c^T-\lambda^TA\geq 0 \implies x^*=0\implies L(\lambda)=\lambda^Tb=b^T\lambda
$$
Quindi
$$
\implies \begin{align}
\text{max } b^T\lambda \\
\lambda^TA\leq c^T
\end{align}
$$
$2^o$ caso:
Esiste $i$ t.c.
$$
c_{i}-\lambda^TA_{i} < 0 \implies L(\lambda) = - \infty
$$
### Esempio di costruzione del duale
$$
\begin{align}
\text{min } 5x_{1}-3x_{2}+3x_{3}-4x_{4}+5x_{5}-6x_{6} \\
\lambda_{1}\to 2x_{1}-3x_{2}-x_{3}+x_{4}-5x_{5}+x_{6}=3 \\
\lambda_{2}\to -x_{1}+x_{2}+2x_{3}+2x_{4}-3x_{5}-x_{6}=1 \\
x\geq 0
\end{align}
$$
Costruiamo il duale:
$$
\begin{align}
\text{max } 3\lambda_{1}+\lambda_{2} \\
2\lambda_{1}-\lambda_{2}\leq 5 \\
-3\lambda_{1}+\lambda_{2}\leq-3 \\
-\lambda_{1}+\lambda_{2}\leq 3 \\
\lambda_{1}+2\lambda_{2}\leq -4 \\
-5\lambda_{1}-3\lambda_{2}\leq 5 \\
\lambda_{1}-\lambda_{2}\leq-6
\end{align}
$$
notiamo che si scambiano i numeri dei vincoli con il numero di variabili.